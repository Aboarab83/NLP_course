# -*- coding: utf-8 -*-
"""BBC_new_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CDBP2A7ssElGP91IK_1T-K9t0s3bJZcW
"""

import spacy
import sklearn
import pandas as pd
import kagglehub
import os

path = kagglehub.dataset_download("moazeldsokyx/bbc-news")

print(os.listdir(path))

df=pd.read_csv( path+"/bbc-text.csv")

df.head()

# let us convert the category into a number format because the ML does not understand text
# same for text

index,labels=pd.factorize(df['category'])
y=index

#let us check the balance of the data
print(df['category'].value_counts())

# @title
# now for the word embedding using tfidf
# I will use sklearn pipeline and multinominal
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,classification_report
Xtrain,Xtest,ytrain,ytest=train_test_split(df['text'],y,test_size=0.3,random_state=4)
model=Pipeline([('vectorizer',TfidfVectorizer()),('classifier',MultinomialNB())])
model.fit(Xtrain,ytrain)
y_pred=model.predict(Xtest)
print(classification_report(ytest,y_pred))

# let us use gridsearchcv with multi models

from sklearn.model_selection import GridSearchCV
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score,classification_report
# create your pipeline with placeholder for the estimator or model
pipeline=Pipeline([("vectorizer",TfidfVectorizer()),("classifier",LinearSVC())])


param_grid=[
    {"classifier":[MultinomialNB()],"classifier__alpha":[0.1,1],"vectorizer__ngram_range":[(1,1),(2,2)]},  # all the classifer has C as regularization except the naive bayes it has alpha
    {"classifier":[LinearSVC()],"classifier__C":[0.1,1,10],"vectorizer__ngram_range":[(1,1),(2,2)]},
    {"classifier":[LogisticRegression()],"classifier__C":[0.1,1,10],"vectorizer__ngram_range":[(1,1),(2,2)]}
    ]


# build your model using gridsearchcv

model=GridSearchCV(pipeline,param_grid,cv=5,n_jobs=-1,verbose=1)
model.fit(Xtrain,ytrain)
print(model.best_estimator_)
y_pred=model.predict(Xtest)

print(classification_report(ytest,y_pred))

# testing our model
print(ytest[:10],y_pred[:10])